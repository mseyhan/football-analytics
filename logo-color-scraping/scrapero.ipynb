{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FOOTBALL LOGO SCRAPER\n",
    "##### For more compelling and effortless visualization applications, I had an idea to scrape the logos and getting a color palette from them to find the most dominant colors. <br> This will help me get the team-relevant coloring for various purposes. <br> I'll use Wikipedia to scrape the logos then ColorThief and Pillow to get the color palette."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.79 s, sys: 69.6 ms, total: 1.86 s\n",
      "Wall time: 302 ms\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>competition_name</th>\n",
       "      <th>home_team_name</th>\n",
       "      <th>home_team_country_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1. Bundesliga</td>\n",
       "      <td>Augsburg</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1. Bundesliga</td>\n",
       "      <td>Bayer Leverkusen</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1. Bundesliga</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1. Bundesliga</td>\n",
       "      <td>Borussia Dortmund</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1. Bundesliga</td>\n",
       "      <td>Borussia Mönchengladbach</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>UEFA Euro</td>\n",
       "      <td>Wales</td>\n",
       "      <td>Wales</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>UEFA Europa League</td>\n",
       "      <td>Bayern Munich</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>UEFA Europa League</td>\n",
       "      <td>Juventus</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>235</th>\n",
       "      <td>UEFA Europa League</td>\n",
       "      <td>Napoli</td>\n",
       "      <td>Italy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>236</th>\n",
       "      <td>UEFA Europa League</td>\n",
       "      <td>VfB Stuttgart</td>\n",
       "      <td>Germany</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>237 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       competition_name            home_team_name home_team_country_name\n",
       "0         1. Bundesliga                  Augsburg                Germany\n",
       "1         1. Bundesliga          Bayer Leverkusen                Germany\n",
       "2         1. Bundesliga             Bayern Munich                Germany\n",
       "3         1. Bundesliga         Borussia Dortmund                Germany\n",
       "4         1. Bundesliga  Borussia Mönchengladbach                Germany\n",
       "..                  ...                       ...                    ...\n",
       "232           UEFA Euro                     Wales                  Wales\n",
       "233  UEFA Europa League             Bayern Munich                Germany\n",
       "234  UEFA Europa League                  Juventus                  Italy\n",
       "235  UEFA Europa League                    Napoli                  Italy\n",
       "236  UEFA Europa League             VfB Stuttgart                Germany\n",
       "\n",
       "[237 rows x 3 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "# SELECTs events from previously recorded event data in a staging table\n",
    "# connect & read\n",
    "conn = sqlite3.connect( \"/Users/muratseyhan/football-analytics/data/statsbomb.db\")\n",
    "# the events of matches\n",
    "\n",
    "with open('/Users/muratseyhan/football-analytics/data/full_season_matches.sql') as inserts:\n",
    "    q1 = inserts.read()\n",
    "df_scrape = pd.read_sql_query('''\n",
    "SELECT   m.competition_name, home_team_name, home_team_country_name\n",
    "FROM MATCH m\n",
    "LEFT JOIN competition c\n",
    "ON c.competition_id = m.competition_id\n",
    "WHERE c.competition_gender = 'male'\n",
    "GROUP BY m.competition_name, home_team_country_name, home_team_name\n",
    "\n",
    "UNION\n",
    "\n",
    "SELECT   m.competition_name, away_team_name, away_team_country_name\n",
    "FROM MATCH m\n",
    "LEFT JOIN competition c\n",
    "ON c.competition_id = m.competition_id\n",
    "WHERE c.competition_gender = 'male'\n",
    "GROUP BY m.competition_name, away_team_country_name, away_team_name\n",
    "ORDER BY 1,2,3\n",
    "''', conn)\n",
    "\n",
    "# close the connection\n",
    "conn.close()\n",
    "df_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import time\n",
    "\n",
    "def get_wikipedia_main_page_link(search_query):\n",
    "    \"\"\"\n",
    "    This function takes a search query, performs a Google search,\n",
    "    and returns the first main Wikipedia page link found in the search results.\n",
    "    It specifically filters out non-article links such as images or categories.\n",
    "    \"\"\"\n",
    "    # Google Search URL\n",
    "    google_search_url = 'https://www.google.com/search'\n",
    "    params = {'q': search_query}\n",
    "\n",
    "    # Perform the Google search\n",
    "    response = requests.get(google_search_url, params=params)\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    # Find all links in the search result\n",
    "    for link in soup.find_all('a'):\n",
    "        href = link.get('href')\n",
    "\n",
    "        # Check if the link is a main Wikipedia page link and return it\n",
    "        if 'en.wikipedia.org/wiki/' in href:\n",
    "            # Exclude links to non-article pages (images, categories, files)\n",
    "            if not re.search(r'/wiki/(File|Category|Portal):', href):\n",
    "                # Extract the actual link from the Google redirect URL\n",
    "                start = href.find('http')  # Find the start of the actual URL\n",
    "                end = href.find('&', start)  # Find the end of the actual URL\n",
    "                wikipedia_link = href[start:end]\n",
    "                return wikipedia_link\n",
    "\n",
    "    # Return None if no main Wikipedia page link is found\n",
    "    return None\n",
    "\n",
    "print(len(df_scrape.home_team_name))\n",
    "\n",
    "# Example usage\n",
    "search_queries = list(\"Football club soccer \" +df_scrape.home_team_name + \" , \" +df_scrape.home_team_country_name+ \" logo\")\n",
    "zib = zip(df_scrape.home_team_name,df_scrape.home_team_country_name)\n",
    "linklist = list()\n",
    "nonelist = list()\n",
    "# Iterate over the search queries and get Wikipedia main page links\n",
    "for query in zib:\n",
    "    search = \"Football club soccer \" + query[0] + \" , \" + query[1] + \" logo\"\n",
    "    wiki_link = get_wikipedia_main_page_link(search)\n",
    "    tup = (query[0],wiki_link)\n",
    "    linklist.append(tup) if wiki_link != None else nonelist.append(tup)\n",
    "    print(f\"Query: {search}, Wikipedia Main Page Link: {wiki_link}\")\n",
    "    time.sleep(.3)  # Sleep to avoid overwhelming Google with requests\n",
    "\n",
    "linklist[:10], len(nonelist), nonelist[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# List of links\n",
    "teams_links = linklist\n",
    "\n",
    "# Function to extract image URLs from Wikipedia pages\n",
    "def get_wikipedia_image_url(page_url):\n",
    "    response = requests.get(page_url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "    infobox = soup.find('table', {'class': 'infobox'})\n",
    "    if infobox:\n",
    "        image = infobox.find('img')\n",
    "        if image:\n",
    "            return 'https:' + image['src']\n",
    "    return None\n",
    "\n",
    "# Function to download image from a URL\n",
    "def download_image(image_url, file_name):\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.3'}\n",
    "    response = requests.get(image_url, headers=headers)\n",
    "    if response.status_code == 200:\n",
    "        with open(file_name, 'wb') as file:\n",
    "            file.write(response.content)\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Directory to save images\n",
    "image_dir = 'data/logos'\n",
    "os.makedirs(image_dir, exist_ok=True)\n",
    "\n",
    "# Structured data\n",
    "structured_data = []\n",
    "\n",
    "for team, link in teams_links:\n",
    "    data = {'team_name': team, 'original_link': link, 'image_url': None, 'download_status': False, 'file_path': None}\n",
    "    if 'wikipedia.org/wiki' in link:\n",
    "        image_url = get_wikipedia_image_url(link)\n",
    "    else:\n",
    "        image_url = link.split('imgurl=')[1].split('&')[0]\n",
    "        image_url = requests.utils.unquote(image_url)\n",
    "\n",
    "    data['image_url'] = image_url\n",
    "    if image_url:\n",
    "        file_extension = image_url.split('.')[-1].split('?')[0]\n",
    "        sanitized_team_name = team.replace(' ', '_')  # Replace spaces with underscores or any other sanitization you need\n",
    "        file_name = os.path.join(image_dir, sanitized_team_name + '.' + file_extension)\n",
    "        data['download_status'] = download_image(image_url, file_name)\n",
    "        data['file_path'] = file_name if data['download_status'] else None\n",
    "    structured_data.append(data)\n",
    "\n",
    "import json\n",
    "\n",
    "# File to save the structured data\n",
    "output_file = 'team_logos_data.json'\n",
    "\n",
    "# Save structured data to a JSON file\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(structured_data, file, indent=4)\n",
    "\n",
    "print(f\"{len(output_file), }Data saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
